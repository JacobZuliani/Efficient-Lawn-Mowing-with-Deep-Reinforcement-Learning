{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from visualization import visualize_environment\n",
    "from random import randrange\n",
    "from os import startfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class environment():\n",
    "    # this class defines what actions are available, what they do, and how they modify the environment\n",
    "    # this class keeps track of the agents attributes including loss\n",
    "\n",
    "    def __init__(self, agent_position, agent_direction, environment_shape):\n",
    "        # position is a 2 element list containing a coordinate pair [x,y]\n",
    "        # direction is a cardinal direction represented by one of the four characters N, S, E, W\n",
    "        # environment_shape is the shape of the environment matrix\n",
    "        \n",
    "        self.environment_shape = environment_shape\n",
    "        self.agent_position = agent_position\n",
    "        self.agent_direction = agent_direction\n",
    "        self.moves = 0\n",
    "        \n",
    "    def __can_occupy(self, tile):\n",
    "        # can the agent occupy tile?\n",
    "                \n",
    "        return tile[0] != '9' # True if tile is not wood\n",
    "    \n",
    "    def __cut_the_grass(self, environment_state):        \n",
    "        # if the current space is tall grass then cut it\n",
    "        if environment_state[self.agent_position[0], self.agent_position[1]][0] == '8': # if current space is tall grass\n",
    "            current_tile = environment_state[self.agent_position[0], self.agent_position[1]]\n",
    "            environment_state[self.agent_position[0], self.agent_position[1]] = self.__modify_tile(current_tile, '1', 0) # replace with cut grass     \n",
    "        return environment_state\n",
    "    \n",
    "    def __modify_tile(self, tile, new_value, position):\n",
    "        # returns tile with new_value in position\n",
    "        # \"why dont you just use arrays instead of non mutable strings?\", mainly cause it makes one hot encoding easier but also cause\n",
    "        # I already did it this way and am only now realizing making a tensor of the lawn instead of the matrix may bave been the better way.\n",
    "        \n",
    "        # converting to an array is slow with numpy.fromstring and slow with python list() so I decided to just not convert it\n",
    "        if position == 0:\n",
    "            return new_value + tile[1] + tile[2]\n",
    "        elif position == 1:\n",
    "            return tile[0] + new_value + tile[2] \n",
    "        elif position == 2:\n",
    "            return tile[0] + tile[1] + new_value\n",
    "        else:\n",
    "            raise Exception('invalid tile index ' + position)\n",
    "        \n",
    "    def take_action(self, action, environment_state):\n",
    "        # takes the action that was recived and returns an updated environment state\n",
    "        \n",
    "        if action == 1:\n",
    "            updated_environment_state = self.advance(environment_state)\n",
    "        elif action == 2:\n",
    "            updated_environment_state = self.pivot_clockwise(environment_state)\n",
    "        elif action == 3:\n",
    "            updated_environment_state = self.pivot_counterclockwise(environment_state)\n",
    "        else:\n",
    "            raise Exception('invalid action_id ' + action)\n",
    "        self.moves += 1        \n",
    "        return updated_environment_state\n",
    "    \n",
    "    # below I define all actions the agent can take and how they modify the environment\n",
    "    def advance(self, environment_state):\n",
    "        # advance one space in the direction the agent is currently facing\n",
    "        # return updated environment state\n",
    "        \n",
    "        current_tile = environment_state[self.agent_position[0], self.agent_position[1]] # defining here for readability\n",
    "        \n",
    "        if self.agent_direction == 'N':\n",
    "            environment_state[self.agent_position[0], self.agent_position[1]] = self.__modify_tile(current_tile, '1', 2) # exit from north\n",
    "            if self.agent_position[0]-1 != -1 and self.__can_occupy(environment_state[self.agent_position[0]-1, self.agent_position[1]]): # if next space can be occupied\n",
    "                self.agent_position[0] -= 1 # advance\n",
    "                current_tile = environment_state[self.agent_position[0], self.agent_position[1]] # redefine current tile since the agent moved\n",
    "                environment_state[self.agent_position[0], self.agent_position[1]] = self.__modify_tile(current_tile, '2', 1) # exit from north # enter from south\n",
    "        elif self.agent_direction == 'S':\n",
    "            environment_state[self.agent_position[0], self.agent_position[1]] = self.__modify_tile(current_tile, '2', 2) # exit from south\n",
    "            if self.agent_position[0]+1 != self.environment_shape[0] and self.__can_occupy(environment_state[self.agent_position[0]+1, self.agent_position[1]]): # if next space can be occupied\n",
    "                self.agent_position[0] += 1 # advance\n",
    "                current_tile = environment_state[self.agent_position[0], self.agent_position[1]] # redefine current tile since the agent moved\n",
    "                environment_state[self.agent_position[0], self.agent_position[1]] = self.__modify_tile(current_tile, '1', 1) # enter from north\n",
    "        elif self.agent_direction == 'E':\n",
    "            environment_state[self.agent_position[0], self.agent_position[1]] = self.__modify_tile(current_tile, '3', 2) # exit from east\n",
    "            if self.agent_position[1]+1 != self.environment_shape[1] and self.__can_occupy(environment_state[self.agent_position[0], self.agent_position[1]+1]): # if next space can be occupied\n",
    "                self.agent_position[1] += 1 # advance\n",
    "                current_tile = environment_state[self.agent_position[0], self.agent_position[1]] # redefine current tile since the agent moved\n",
    "                environment_state[self.agent_position[0], self.agent_position[1]] = self.__modify_tile(current_tile, '4', 1) # enter from west\n",
    "        elif self.agent_direction == 'W':\n",
    "            environment_state[self.agent_position[0], self.agent_position[1]] = self.__modify_tile(current_tile, '4', 2) # exit from west\n",
    "            if self.agent_position[1]-1 != -1 and self.__can_occupy(environment_state[self.agent_position[0], self.agent_position[1]-1]): # if next space can be occupied\n",
    "                self.agent_position[1] -= 1 # advance\n",
    "                current_tile = environment_state[self.agent_position[0], self.agent_position[1]] # redefine current tile since the agent moved\n",
    "                environment_state[self.agent_position[0], self.agent_position[1]] = self.__modify_tile(current_tile, '3', 1) # enter from east\n",
    "        else:\n",
    "            raise Exception('unknown direction')\n",
    "        environment_state = self.__cut_the_grass(environment_state) # if the current space is tall grass then cut it\n",
    "        return environment_state\n",
    "    \n",
    "    def pivot_clockwise(self, environment_state):\n",
    "        # rotate direction by 90° clockwise\n",
    "        # return updated environment state\n",
    "        \n",
    "        if self.agent_direction == 'N':\n",
    "            self.agent_direction = 'E'\n",
    "        elif self.agent_direction == 'S':\n",
    "            self.agent_direction = 'W'\n",
    "        elif self.agent_direction == 'E':\n",
    "            self.agent_direction = 'S'\n",
    "        elif self.agent_direction == 'W':\n",
    "            self.agent_direction = 'N'\n",
    "        else:\n",
    "            raise Exception('unknown direction')        \n",
    "\n",
    "        return environment_state\n",
    "        \n",
    "    def pivot_counterclockwise(self, environment_state):\n",
    "        # rotate direction by 90° counterclockwise\n",
    "        # return updated environment state\n",
    "        \n",
    "        if self.agent_direction == 'N':\n",
    "            self.agent_direction = 'W'\n",
    "        elif self.agent_direction == 'S':\n",
    "            self.agent_direction = 'E'\n",
    "        elif self.agent_direction == 'E':\n",
    "            self.agent_direction = 'N'\n",
    "        elif self.agent_direction == 'W':\n",
    "            self.agent_direction = 'S'\n",
    "        else:\n",
    "            raise Exception('unknown direction')        \n",
    "        \n",
    "        return environment_state\n",
    "    \n",
    "    # below I define all getters which are just used to interface with the simulation class\n",
    "    def get_done_condition(self, environment_state):\n",
    "        # returns true if the environment is complete (entire lawn is mowed)\n",
    "        \n",
    "        return ('800' not in environment_state) # if there are no tall grass blocks then lawn is mowed\n",
    "    \n",
    "    def get_action_space(self):\n",
    "        # returns tuple of possible actions as numbers\n",
    "        # mapping:\n",
    "        # 1 = 'advance'\n",
    "        # 2 = 'pivot_clockwise'\n",
    "        # 3 = 'pivot_counterclockwise'\n",
    "        \n",
    "        return (1,2,3)\n",
    "        \n",
    "    def get_position(self):\n",
    "        # return agents current position\n",
    "        return self.x, self.y\n",
    "    \n",
    "    def get_direction(self):\n",
    "        # return agents current direction\n",
    "        return self.direction\n",
    "    \n",
    "    def get_reward(self):\n",
    "        # return agents current reward\n",
    "        return self.moves\n",
    "    \n",
    "class simulation():\n",
    "    # this class manages the simulation for any agent or environment\n",
    "\n",
    "    def __init__(self, starting_environment_state, environment, visualization):\n",
    "        # environment is a class instance defining what the agent can and can't do\n",
    "        # visualization is an instance of visualize_environment from visualization.py\n",
    "        # starting_environment_state is the starting view of the environment (matrix of strings)\n",
    "        \n",
    "        # below variables should not be modified\n",
    "        self.starting_visualization = visualization # class instance\n",
    "        self.starting_environment = environment # class instance\n",
    "        self.starting_environment_state = starting_environment_state # matrix of strings\n",
    "        self.reset()\n",
    "        \n",
    "    def get_action_space(self):\n",
    "        # returns a tuple of functions where each function defines the action\n",
    "        \n",
    "        return self.environment.get_action_space()\n",
    "    \n",
    "    def get_simulation_history_visualization(self, file_name, fps):\n",
    "        # outputs an mp4 file of the agent mowing the lawn\n",
    "        \n",
    "        self.visualization.output_history(file_name, fps)\n",
    "        \n",
    "    def display_last_env_state(self):\n",
    "        # displays the last environment state of the simulation\n",
    "        \n",
    "        self.visualization.display_last_environment_state()\n",
    "    \n",
    "    def reset(self):\n",
    "        # resets environment to the way it was when the simulation was initalized\n",
    "        # returns the starting environment state\n",
    "        \n",
    "        # everything needs to be copied (instead of creating another pointer) to avoid modifying the original\n",
    "        self.current_environment_state = deepcopy(self.starting_environment_state)\n",
    "        self.visualization = deepcopy(self.starting_visualization)\n",
    "        self.environment = deepcopy(self.starting_environment)\n",
    "        self.reward = 0\n",
    "        return self.current_environment_state\n",
    "        \n",
    "    def step(self, action):\n",
    "        # competes single environment step where agent takes action, then recives observation, reward, and whether they are done\n",
    "        \n",
    "        self.current_environment_state = self.environment.take_action(action, self.current_environment_state) # take action to get next environment state\n",
    "        self.reward = self.environment.get_reward() # obtain current reward\n",
    "        self.visualization.recive_environment_state(self.current_environment_state) # update visualization\n",
    "        # return updated environment state, reqard, and done_condition\n",
    "        return self.current_environment_state, self.reward, self.environment.get_done_condition(self.current_environment_state)\n",
    "    \n",
    "def show_environment_state(environment_state):\n",
    "    # shows any environment state inde\n",
    "    \n",
    "    visualization = visualize_environment()\n",
    "    visualization.recive_environment_state(starting_environment_state)\n",
    "    visualization.display_last_environment_state(ipynb = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "starting_environment_state = np.array([\n",
    "    ['900','900','800','800','800','800','800','800','800','800','800','800','800','800','800','800','800','800','800', '800', '800', '800'],\n",
    "    ['800','800','800','800','800','800','800','800','800','800','800','800','800','800','800','800','800','800','800', '800', '800', '800'],\n",
    "    ['800','800','800','800','800','800','800','800','800','800','800','800','800','800','800','800','800','800','800', '800', '800', '800'],\n",
    "    ['800','800','800','800','800','800','800','800','800','800','800','800','800','800','800','800','800','800','800', '800', '800', '800'],\n",
    "    ['800','800','800','800','800','800','800','800','800','800','900','900','900','800','800','800','800','800','800', '800', '800', '800'],\n",
    "    ['800','800','800','800','800','800','800','800','800','800','900','900','900','800','800','800','800','800','800', '800', '800', '800'],\n",
    "    ['800','800','800','800','200','200','200','200','900','900','900','900','900','900','900','900','800','800','800', '800', '800', '800'],\n",
    "    ['800','800','800','800','200','200','200','200','900','900','900','900','900','900','900','900','800','800','800', '800', '800', '800'],\n",
    "    ['800','800','800','800','200','200','800','800','100','800','800','800','900','900','900','900','800','800','800', '800', '800', '800'],\n",
    "    ['800','800','800','800','200','200','800','800','800','800','800','800','900','900','900','900','800','800','800', '800', '800', '800'],\n",
    "    ['800','800','800','800','200','200','800','800','800','800','800','800','800','800','800','800','800','800','800', '800', '800', '800'],\n",
    "    ['800','800','800','800','200','200','800','800','800','800','800','800','800','800','800','800','800','800','800', '800', '800', '800'],\n",
    "    ['200','200','200','200','200','200','800','800','800','800','900','900','800','800','800','800','800','800','900', '900', '800', '800'],\n",
    "    ['200','200','200','200','200','200','800','800','800','800','900','900','800','800','800','800','800','800','900', '900', '800', '800'],\n",
    "    ['800','800','800','800','800','800','800','800','800','800','800','800','800','800','800','800','800','800','800', '800', '800', '800'],\n",
    "    ['800','800','800','800','800','800','800','800','800','800','800','800','800','800','800','800','800','800','800', '800', '800', '800']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# used this to test that the done method works\n",
    "starting_environment_state = np.array([\n",
    "    ['900','900','800','800','800','800','800'],\n",
    "    ['800','800','800','800','800','800','800'],\n",
    "    ['800','800','800','800','800','800','800']])\n",
    "#show_environment_state(starting_environment_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim = simulation(starting_environment_state, environment([0,2], 'S', starting_environment_state.shape), visualize_environment())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-dc690a624366>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrandrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# take random action\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0msim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_simulation_history_visualization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"random_agent_small_env.mp4\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m120\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-ad3cf0b9450a>\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    196\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcurrent_environment_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menvironment\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcurrent_environment_state\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# take action to get next environment state\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreward\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menvironment\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_reward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# obtain current reward\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 198\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvisualization\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecive_environment_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcurrent_environment_state\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# update visualization\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    199\u001b[0m         \u001b[1;31m# return updated environment state, reqard, and done_condition\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcurrent_environment_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menvironment\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_done_condition\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcurrent_environment_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\DataScience\\school\\Efficient-Lawn-Mowing-with-Deep-Reinforcement-Learning\\visualization.py\u001b[0m in \u001b[0;36mrecive_environment_state\u001b[1;34m(self, environment_state)\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[1;31m# records the environment state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m         \u001b[0menvironment_state_image_array\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__get_environment_state_image_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menvironment_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menvironment_history\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menvironment_state_image_array\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\DataScience\\school\\Efficient-Lawn-Mowing-with-Deep-Reinforcement-Learning\\visualization.py\u001b[0m in \u001b[0;36m__get_environment_state_image_array\u001b[1;34m(self, environment_state)\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[0menvironment_state_image_as_array\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtile_id\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menvironment_state\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m             \u001b[0menvironment_state_image_as_array\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__get_tile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtile_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m         \u001b[0menvironment_state_image_as_array\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menvironment_state_image_as_array\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m         \u001b[1;31m# convert array shape to (height*nrows, width*ncols, intensity)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\DataScience\\school\\Efficient-Lawn-Mowing-with-Deep-Reinforcement-Learning\\visualization.py\u001b[0m in \u001b[0;36m__get_tile\u001b[1;34m(self, tile_id)\u001b[0m\n\u001b[0;32m    123\u001b[0m             \u001b[0mtile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__construct_tile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moverlay\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m270\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mentry_id\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'2'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# entry = south\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 125\u001b[1;33m             \u001b[0mtile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__construct_tile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moverlay\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m90\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    126\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mentry_id\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'3'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# entry = east\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m             \u001b[0mtile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__construct_tile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moverlay\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m180\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\DataScience\\school\\Efficient-Lawn-Mowing-with-Deep-Reinforcement-Learning\\visualization.py\u001b[0m in \u001b[0;36m__construct_tile\u001b[1;34m(self, background, foreground, foreground_rotation_angle)\u001b[0m\n\u001b[0;32m     59\u001b[0m         \u001b[1;31m# rotates foreground by foreground_rotation_angle (which should be divisible by 90)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m         \u001b[1;31m# then places the foreground image in the center of backround image\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m         \u001b[0mforeground\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mforeground\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrotate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mforeground_rotation_angle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m         \u001b[0mbackground\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpaste\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mforeground\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mforeground\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mbackground\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\PIL\\Image.py\u001b[0m in \u001b[0;36mrotate\u001b[1;34m(self, angle, resample, expand, center, translate, fillcolor)\u001b[0m\n\u001b[0;32m   2007\u001b[0m             \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnh\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2008\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2009\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAFFINE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresample\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfillcolor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfillcolor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2010\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2011\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\PIL\\Image.py\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, size, method, data, resample, fill, fillcolor)\u001b[0m\n\u001b[0;32m   2288\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"RGBa\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2289\u001b[0m                 \u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresample\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfill\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfillcolor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2290\u001b[1;33m                 \u001b[1;33m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"RGBA\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2291\u001b[0m             )\n\u001b[0;32m   2292\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\PIL\\Image.py\u001b[0m in \u001b[0;36mconvert\u001b[1;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[0;32m   1054\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1055\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1056\u001b[1;33m             \u001b[0mim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdither\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1057\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1058\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "observation = sim.reset()\n",
    "\n",
    "done = False\n",
    "while not done:\n",
    "    observation, reward, done = sim.step(randrange(1,4)) # take random action\n",
    "print(reward)\n",
    "sim.get_simulation_history_visualization(\"random_agent.mp4\", 120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "startfile('random_agent.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim.get_simulation_history_visualization(\"random_agent.mp4\", 120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
